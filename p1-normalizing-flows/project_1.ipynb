{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import nf_utils as nf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Running on device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1, part 1: Normalizing Flows (100 pt)\n",
    "In this notebook we will implement two normalizing flow (NF) layers, stack tranformations, and train the model with maximum likelihood on 3 datasets.\n",
    "\n",
    "## Your task\n",
    "Complete the missing code. Make sure that all the functions follow the provided specification, i.e. the output of the function exactly matches the description in the docstring. \n",
    "\n",
    "Do not add or modify any code outside of the following comment blocks\n",
    "```\n",
    "##########################################################\n",
    "# YOUR CODE HERE\n",
    ".....\n",
    "##########################################################\n",
    "```\n",
    "After you fill in all the missing code, restart the kernel and re-run all the cells in the notebook.\n",
    "\n",
    "The following things are **NOT** allowed:\n",
    "- Using additional `import` statements\n",
    "- Using `torch.autograd.functional.jacobian`\n",
    "- Using `torch.det`\n",
    "- Using `torch.distributions`\n",
    "- Copying / reusing code from other sources (e.g. code by other students)\n",
    "\n",
    "If you plagiarize even for a single project task, you won't be eligible for the bonus this semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Normalizing Flow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All transformations (i.e. normalizing flow layers) inherit from the class `nf.Flow` that is included in `nf_utils.py`. The class `nf.Flow` has the following methods:\n",
    "- `forward`: Apply the transformation and compute the Jacobian determinant (Slide 31)\n",
    "- `inverse`: Apply the inverse transformation (if it exists in closed form) and compute the Jacobian determinant of the *inverse* transformation (Slide 27)\n",
    "\n",
    "Additonally, by calling `Flow.get_inverse()` we \"reverse the direction\" of the transformation. That is, the foward transformation becomes the inverse, and vice versa..\n",
    "\n",
    "In this section, we will implement two NF transformations:\n",
    "- Affine transformation\n",
    "- Radial transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine tranformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An affine tranformation scales each dimension independently and shifts the inputs by a constant offset. The tranformation is defined as follows:\n",
    "\n",
    "$$f(\\mathbf{z})= \\exp(\\mathbf{a}) \\odot \\mathbf{z} + \\mathbf{b}$$\n",
    "\n",
    "where parameters $\\mathbf{a} \\in \\mathbb{R}^{D}$ and $\\mathbf{b} \\in \\mathbb{R}^{D}$. \n",
    "We apply $\\exp$ elementwise to $\\mathbf{a}$ to obtain positive scales for each dimension. \n",
    "\n",
    "Note that we can compute the inverse of this transformation analytically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Affine - forward (10 pt)\n",
    "Implement the `forward` method in the class `Affine`.\n",
    "\n",
    "\n",
    "#### Task 2: Affine - inverse (10 pt)\n",
    "Implement the `inverse` method in the class `Affine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine(nf.Flow):\n",
    "    \"\"\"Affine transformation y = e^a * x + b.\n",
    "    \n",
    "    Args:\n",
    "        dim (int): dimension of input/output data. int\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 2):\n",
    "        \"\"\" Create and init an affine transformation. \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.log_scale = nn.Parameter(torch.zeros(self.dim))  # a\n",
    "        self.shift = nn.Parameter(torch.zeros(self.dim))  # b\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Compute the forward transformation given an input x.\n",
    "\n",
    "        Args:\n",
    "            x: input sample. shape [batch_size, dim]\n",
    "\n",
    "        Returns:\n",
    "            y: sample after forward tranformation. shape [batch_size, dim]\n",
    "            log_det_jac: log determinant of the jacobian of the forward tranformation, shape [batch_size]\n",
    "        \"\"\"\n",
    "        B, D = x.shape\n",
    "        \n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        ##########################################################\n",
    "        \n",
    "        assert y.shape == (B, D)\n",
    "        assert log_det_jac.shape == (B,)\n",
    "\n",
    "        return y, log_det_jac\n",
    "\n",
    "    def inverse(self, y: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Compute the inverse transformation given an input y.\n",
    "\n",
    "        Args:\n",
    "            y: input sample. shape [batch_size, dim]\n",
    "\n",
    "        Returns:\n",
    "            x: sample after inverse tranformation. shape [batch_size, dim]\n",
    "            inv_log_det_jac: log determinant of the jacobian of the inverse tranformation, shape [batch_size]\n",
    "        \"\"\"\n",
    "        B, D = y.shape\n",
    "\n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        ##########################################################\n",
    "\n",
    "        assert x.shape == (B, D)\n",
    "        assert inv_log_det_jac.shape == (B,)\n",
    "\n",
    "        return x, inv_log_det_jac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial tranformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A radial flow is a simple but expressive transformation. It has been introduced in this [paper](https://arxiv.org/pdf/1505.05770.pdf) by Rezende and Mohamed. The transformation is defined as:\n",
    "\n",
    "$$f(\\mathbf{z}) =  \\mathbf{z} + \\beta h(\\alpha, r)(\\mathbf{z} - \\mathbf{z_0})$$\n",
    "\n",
    "where $r= \\|\\mathbf{z} - \\mathbf{z_0}\\|_2$, $h(\\alpha, r) = \\frac{1}{\\alpha + r}$ and parameters $\\mathbf{z_0} \\in \\mathbb{R}^{D}$, $\\alpha \\in \\mathbb{R}_+$ and $\\beta \\in \\mathbb{R}$. The parameters need to satisfy the constraints $\\alpha > 0$ and $\\beta \\geq -\\alpha$ for the transformation to be invertible. \n",
    "\n",
    "To enfore the above constraints in practice, we can do the following.\n",
    "\n",
    "- Define a learnable parameter $\\tilde{\\alpha} \\in \\mathbb{R}$ and obtain $\\alpha$ for the transformation as $\\alpha = \\textrm{softplus}(\\tilde{\\alpha})$.\n",
    "- Define a learnable parameter $\\tilde{\\beta} \\in \\mathbb{R}$ and obtain $\\beta$ for the transformation as $\\beta =-\\alpha + \\textrm{softplus}(\\tilde{\\beta}) > -\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Radial - forward (25 pt)\n",
    "Implement the `forward` method in the class `Radial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Radial(nf.Flow):\n",
    "    \"\"\"Radial transformation.\n",
    "    \n",
    "    Args:\n",
    "        dim: dimension of input/output data, int\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 2):\n",
    "        \"\"\" Create and initialize an affine transformation. \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        \n",
    "        self.x0 = nn.Parameter(torch.Tensor(self.dim,)) # Vector used to parametrize z_0 \n",
    "        self.pre_alpha = nn.Parameter(torch.Tensor(1,)) # Scalar used to indirectly parametrized \\alpha\n",
    "        self.pre_beta = nn.Parameter(torch.Tensor(1,)) # Scaler used to indireclty parametrized \\beta\n",
    "        \n",
    "        stdv = 1. / np.sqrt(self.dim)\n",
    "        self.pre_alpha.data.uniform_(-stdv, stdv)\n",
    "        self.pre_beta.data.uniform_(-stdv, stdv)\n",
    "        self.x0.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Compute the forward transformation for the given input x.\n",
    "        \n",
    "        Args:\n",
    "            x: input sample, shape [batch_size, dim]\n",
    "\n",
    "        Returns:\n",
    "            y: sample after forward tranformation, shape [batch_size, dim]\n",
    "            log_det_jac: log determinant of the jacobian of the forward tranformation, shape [batch_size]\n",
    "        \"\"\"\n",
    "        B, D = x.shape\n",
    "\n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        ##########################################################\n",
    "\n",
    "        assert y.shape == (B, D)\n",
    "        assert log_det_jac.shape == (B,)\n",
    "\n",
    "        return y, log_det_jac\n",
    "    \n",
    "    def inverse(self, y: Tensor) -> None:\n",
    "        \"\"\"Compute the inverse transformation given an input y.\n",
    "\n",
    "        Args:\n",
    "            y: input sample. shape [batch_size, dim]\n",
    "\n",
    "        Returns:\n",
    "            x: sample after inverse tranformation. shape [batch_size, dim]\n",
    "            inv_log_det_jac: log determinant of the jacobian of the inverse tranformation, shape [batch_size]\n",
    "        \"\"\"\n",
    "        raise ValueError(\"The inverse tranformation is not known in closed form.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking tranformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a more expressive NF model, we need to stack multiple transformations. It's important that every stacked transformation can be computed in the `forward` direction if we want to perform sampling (slide 31). Similarly, we need to be able to compute the `inverse` direction for each transformation if we want to learn the model via MLE (slide 27)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: stacking tranformations - log_prob (20 pt)\n",
    "Implement the method `log_prob` in class `StackedFlows`. This method should compute tthe log density for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: stacking tranformations - rsample (20 pt)\n",
    "Implement the method `rsample` in class `StackedFlows`. This method should draw a sample from the base distribution, pass it through all the transformations and compute its log density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedFlows(nn.Module):\n",
    "    \"\"\"Stack a list of tranformations with a given based distribtuion.\n",
    "\n",
    "    Args:\n",
    "        tranforms: list fo stacked tranformations. list of Flows\n",
    "        dim: dimension of input/output data. int\n",
    "        base_dist: name of the base distribution. options: ['Normal']\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        transforms: List[nf.Flow], \n",
    "        dim: int = 2, \n",
    "        base_dist: str = 'Normal'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(transforms, nf.Flow):\n",
    "            self.transforms = nn.ModuleList([transforms, ])\n",
    "        elif isinstance(transforms, list):\n",
    "            if not all(isinstance(t, nf.Flow) for t in transforms):\n",
    "                raise ValueError(\"transforms must be a Flow or a list of Flows\")\n",
    "            self.transforms = nn.ModuleList(transforms)\n",
    "        else:\n",
    "            raise ValueError(f\"transforms must a Flow or a list, but was {type(transforms)}\")\n",
    "            \n",
    "        self.dim = dim\n",
    "        if base_dist == \"Normal\":\n",
    "            self.base_dist = MultivariateNormal(torch.zeros(self.dim).to(device), torch.eye(self.dim).to(device))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def log_prob(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Compute log probability of a batch of data (slide 27).\n",
    "\n",
    "        Args:\n",
    "            x: input sample. shape [batch_size, dim]\n",
    "\n",
    "        Returns:\n",
    "            log_prob: Log probability of the data, shape [batch_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        B, D = x.shape\n",
    "\n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        ##########################################################\n",
    "        \n",
    "        assert log_prob.shape == (B,)\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def rsample(self, batch_size: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Sample from the transformed distribution (slide 31).\n",
    "\n",
    "        Returns:\n",
    "            x: sample after forward tranformation, shape [batch_size, dim]\n",
    "            log_prob: Log probability of x, shape [batch_size]\n",
    "        \"\"\"\n",
    "        ##########################################################\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        ##########################################################\n",
    "\n",
    "        assert x.shape == (batch_size, self.dim)\n",
    "        assert log_prob.shape == (batch_size,)\n",
    "\n",
    "        return x, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Maximum-likelihood training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train normalizing flows by maximizing the log-likelihood (Slide 28) of the observed data points $\\boldsymbol{x}^{(i)}$ w.r.t. the flow parameters $\\varphi$ i.e.:\n",
    "\n",
    "$$\\max_\\varphi \\frac{1}{|\\mathcal{D}|} \\sum_{\\boldsymbol{x}^{(i)} \\in \\mathcal{D}} \\log p(\\boldsymbol{x}^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: training - maximum likelihood (15 pt)\n",
    "Complete the functions train such that it trains the model with maximum likelihood. \n",
    "\n",
    "The variable loss should be a scalar equal to the the mean loss for the data in the current batch. Note that here we expect to minimize the negaitve log-likelihood instead of maximizing the log-likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size=100, max_epochs=1000, frequency=250):\n",
    "    \"\"\"Train a normalizing flow model with maximum likelihood.\n",
    "\n",
    "    Args:\n",
    "        model: normalizing flow model. Flow or StackedFlows\n",
    "        dataset: dataset containing data to fit. Dataset\n",
    "        batch_size: number of samples per batch. int\n",
    "        max_epochs: number of training epochs. int\n",
    "        frequency: frequency for plotting density visualization. int\n",
    "        \n",
    "    Return:\n",
    "        model: trained model. Flow or StackedFlows\n",
    "        losses: loss evolution during training. list of floats\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Train model\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "    for epoch in range(max_epochs + 1):\n",
    "        total_loss = 0\n",
    "        for batch_index, (X_train) in enumerate(train_loader):\n",
    "            ##########################################################\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            ##########################################################\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(train_loader)\n",
    "        losses.append(total_loss)\n",
    "        \n",
    "        if epoch % frequency == 0:\n",
    "            print(f\"Epoch {epoch} -> loss: {total_loss:.2f}\")\n",
    "            nf.plot_density(model, train_loader, device=device)\n",
    "    \n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit our models on the following toy datasets:\n",
    "- A single Gaussian with non-zero mean\n",
    "- Three gaussians\n",
    "- Two moons\n",
    "\n",
    "For each dataset, we train an affine and a radial transformation with a Gaussian base distribution. The affine tranformation should only be able to scale and shift the base distribution. The radial tranformation is capable of more complex transformations.\n",
    "\n",
    "Plots show:\n",
    "- Evolution of density estimation during training.\n",
    "- The loss curve during training. \n",
    "- The density learned by the model after training.\n",
    "- Samples from the model after training (if possible).\n",
    "\n",
    "If everything is implement correctly, you should see significant changes in the loss value after the first 100 epochs for all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: shifted Gaussian\n",
    "The first dataset composed of one Gaussian with a non zero mean. All flows should manage to fit this density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = nf.CircleGaussiansDataset(n_gaussians=1, n_samples=500)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(dataset_1.X[:,0], dataset_1.X[:,1], alpha=.05, marker='x', c='C1')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine flow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good final training loss is < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms = [Affine()]\n",
    "model = StackedFlows(transforms, base_dist='Normal').to(device)\n",
    "model, losses = train(model, dataset_1, max_epochs=201)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "nf.plot_density(model, [], device=device)\n",
    "nf.plot_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial flow (4 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms = [Radial().get_inverse().to(device) for _ in range(4)]\n",
    "model = StackedFlows(transforms, base_dist='Normal').to(device)\n",
    "model, losses = train(model, dataset_1, max_epochs=501)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "nf.plot_density(model, [], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: 3 Gaussians\n",
    "The second dataset is composed of 3 gaussians with means on a circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = nf.CircleGaussiansDataset(n_gaussians=3, n_samples=400, variance=.4)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(dataset_2.X[:,0], dataset_2.X[:,1], alpha=.05, marker='x', c='C1')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine flow should fail here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms = [Affine().to(device)]\n",
    "model = StackedFlows(transforms, base_dist='Normal').to(device)\n",
    "model, losses = train(model, dataset_2, max_epochs=201)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses, marker='*')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "nf.plot_density(model, [], device=device)\n",
    "nf.plot_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial flow (16 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 16 layers of radial flow should lead to a good reasonable fit of the data after 500 epochs. Traning with more layers and for more epochs would improve the density estimation but would take more time. You might have to run the training multiple times to learn the three Gaussians (sometimes only two Gaussians are learned by the flow).\n",
    "\n",
    "Good loss is < 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms = [Radial().get_inverse() for _ in range(16)]\n",
    "model = StackedFlows(transforms, base_dist='Normal').to(device)\n",
    "model, losses = train(model, dataset_2, max_epochs=501, frequency=100)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "nf.plot_density(model, [], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3: 2 Moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third dataset is composed of 2 moons. Affine flow should fail again. With more layers, Radial flow should work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3 = nf.MoonsDataset()\n",
    "plt.scatter(dataset_3.X[:,0], dataset_3.X[:,1], alpha=.05, marker='x', c='orange')\n",
    "plt.xlim(-2.5, 3)\n",
    "plt.ylim(-2.5, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms = [Affine().to(device)]\n",
    "model = StackedFlows(transforms, base_dist='Normal').to(device)\n",
    "model, losses = train(model, dataset_3, max_epochs=500)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses, marker='*')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "nf.plot_density(model, [], device=device)\n",
    "nf.plot_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine flow should fail here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial flow (16 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 16 layers of radial flow should lead to a good reasonable fit of the data after 500 epochs. Traning with more layers and for more epochs would improve the density estimation but would take more time.\n",
    "\n",
    "Good loss is < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [Radial().get_inverse().to(device) for _ in range(16)]\n",
    "model = StackedFlows(transforms, base_dist='Normal').to(device)\n",
    "model, losses = train(model, dataset_3, max_epochs=501, frequency=100)\n",
    "\n",
    "# Plots\n",
    "plt.plot(losses, marker='*')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "nf.plot_density(model, [], mesh_size=3, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
